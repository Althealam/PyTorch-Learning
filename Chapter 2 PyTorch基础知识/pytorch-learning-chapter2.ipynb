{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Chapter 2 PyTorch基础知识","metadata":{}},{"cell_type":"markdown","source":"* 0维张量/标量：数字\n* 1维张量/向量：向量\n* 2维张量：矩阵\n* 3维张量：时间序列（公用数据集类型）\n* 4维张量：图像\n* 5维张量：视频","metadata":{}},{"cell_type":"markdown","source":"eg: (width,height,channel)=3D<br>\n    (sample_size,width,height,channel)=4D","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-07-12T06:33:57.503655Z","iopub.execute_input":"2023-07-12T06:33:57.504058Z","iopub.status.idle":"2023-07-12T06:33:57.508627Z","shell.execute_reply.started":"2023-07-12T06:33:57.504028Z","shell.execute_reply":"2023-07-12T06:33:57.507654Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### 创建tensor","metadata":{}},{"cell_type":"code","source":"#创建一个随机初始化4行3列的矩阵\nx=torch.rand(4,3)\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T06:33:59.814891Z","iopub.execute_input":"2023-07-12T06:33:59.815247Z","iopub.status.idle":"2023-07-12T06:33:59.820339Z","shell.execute_reply.started":"2023-07-12T06:33:59.815218Z","shell.execute_reply":"2023-07-12T06:33:59.819678Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tensor([[0.9615, 0.0910, 0.7541],\n        [0.5478, 0.9582, 0.1062],\n        [0.1827, 0.2619, 0.0581],\n        [0.4111, 0.8922, 0.3651]])\n","output_type":"stream"}]},{"cell_type":"code","source":"#构造一个矩阵全为0，并且数据类型为long\nx=torch.zeros(4,3,dtype=torch.long)\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T06:34:03.953077Z","iopub.execute_input":"2023-07-12T06:34:03.953404Z","iopub.status.idle":"2023-07-12T06:34:03.960378Z","shell.execute_reply.started":"2023-07-12T06:34:03.953383Z","shell.execute_reply":"2023-07-12T06:34:03.959107Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"tensor([[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])\n","output_type":"stream"}]},{"cell_type":"code","source":"#使用已有的数据，构造一个张量\nx=torch.tensor([5.5,3])\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T06:34:06.075493Z","iopub.execute_input":"2023-07-12T06:34:06.075908Z","iopub.status.idle":"2023-07-12T06:34:06.082814Z","shell.execute_reply.started":"2023-07-12T06:34:06.075875Z","shell.execute_reply":"2023-07-12T06:34:06.081855Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"tensor([5.5000, 3.0000])\n","output_type":"stream"}]},{"cell_type":"code","source":"#基于已经存在的tensor，创建一个tensor\nx=x.new_ones(4,3,dtype=torch.double)\n#或者写为:x=torch.ones(4,3,dtype=torch.double)\nprint(x)\n\n#重置数据类型\nx=torch.randn_like(x,dtype=torch.float)\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T06:34:43.682355Z","iopub.execute_input":"2023-07-12T06:34:43.682714Z","iopub.status.idle":"2023-07-12T06:34:43.693599Z","shell.execute_reply.started":"2023-07-12T06:34:43.682685Z","shell.execute_reply":"2023-07-12T06:34:43.692323Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)\ntensor([[-1.1805,  0.3705, -1.9573],\n        [ 1.8177, -0.1416, -1.6568],\n        [ 1.1019,  1.0646, -0.6532],\n        [ 0.0510, -0.4728, -0.1473]])\n","output_type":"stream"}]},{"cell_type":"code","source":"#获取维度信息\nprint(x.size())\nprint(x.shape)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T06:35:17.197552Z","iopub.execute_input":"2023-07-12T06:35:17.198312Z","iopub.status.idle":"2023-07-12T06:35:17.204075Z","shell.execute_reply.started":"2023-07-12T06:35:17.198256Z","shell.execute_reply":"2023-07-12T06:35:17.203111Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"torch.Size([4, 3])\ntorch.Size([4, 3])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 常见的构造Tensor的函数\n* Tensor(*sizes) 基础构造函数\n* tensor(data) 类似于np.array\n* ones(*sizes) 全1\n* zeros(*sizes) 全0\n* eye(*sizes) 对角为1，其余为0\n* arange(s,e,step) 从s到e，步长为step\n* linspace(s,e,steps) 从s到e，均匀分为step份\n* rand/randn(*sizes) rand是[0,1)的均匀分布，randn是服从N(0,1)的正态分布\n* normal(mean,std) 正态分布（均值为mean，标准差为std）\n* randperm(m) 随机排列","metadata":{}},{"cell_type":"markdown","source":"#### 操作","metadata":{}},{"cell_type":"markdown","source":"##### `加法操作`","metadata":{}},{"cell_type":"code","source":"#方法一\ny=torch.rand(4,3)\nprint(x+y)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T06:45:25.018490Z","iopub.execute_input":"2023-07-12T06:45:25.018833Z","iopub.status.idle":"2023-07-12T06:45:25.029089Z","shell.execute_reply.started":"2023-07-12T06:45:25.018809Z","shell.execute_reply":"2023-07-12T06:45:25.027870Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"tensor([[-3.9194e-01,  1.3294e+00, -1.5366e+00],\n        [ 2.8090e+00,  6.6517e-01, -1.2615e+00],\n        [ 1.1996e+00,  1.0735e+00,  1.1622e-01],\n        [ 3.6831e-01,  1.7502e-03,  7.1452e-01]])\n","output_type":"stream"}]},{"cell_type":"code","source":"#方法二\nprint(torch.add(x,y))","metadata":{"execution":{"iopub.status.busy":"2023-07-12T06:45:37.295324Z","iopub.execute_input":"2023-07-12T06:45:37.295655Z","iopub.status.idle":"2023-07-12T06:45:37.303431Z","shell.execute_reply.started":"2023-07-12T06:45:37.295632Z","shell.execute_reply":"2023-07-12T06:45:37.301944Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"tensor([[-3.9194e-01,  1.3294e+00, -1.5366e+00],\n        [ 2.8090e+00,  6.6517e-01, -1.2615e+00],\n        [ 1.1996e+00,  1.0735e+00,  1.1622e-01],\n        [ 3.6831e-01,  1.7502e-03,  7.1452e-01]])\n","output_type":"stream"}]},{"cell_type":"code","source":"#方法三\nresult=torch.empty(5,3)\ntorch.add(x,y,out=result)\nprint(result)\n#这里的out不需要和真实的运算结果保持维数一致，但是会有警告提示","metadata":{"execution":{"iopub.status.busy":"2023-07-12T06:46:15.713640Z","iopub.execute_input":"2023-07-12T06:46:15.713998Z","iopub.status.idle":"2023-07-12T06:46:15.722310Z","shell.execute_reply.started":"2023-07-12T06:46:15.713965Z","shell.execute_reply":"2023-07-12T06:46:15.720736Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"tensor([[-3.9194e-01,  1.3294e+00, -1.5366e+00],\n        [ 2.8090e+00,  6.6517e-01, -1.2615e+00],\n        [ 1.1996e+00,  1.0735e+00,  1.1622e-01],\n        [ 3.6831e-01,  1.7502e-03,  7.1452e-01]])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_32/70477955.py:3: UserWarning: An output with one or more elements was resized since it had shape [5, 3], which does not match the required output shape [4, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n  torch.add(x,y,out=result)\n","output_type":"stream"}]},{"cell_type":"code","source":"#方法四\ny.add_(x)\nprint(y)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T06:47:01.008260Z","iopub.execute_input":"2023-07-12T06:47:01.008653Z","iopub.status.idle":"2023-07-12T06:47:01.015690Z","shell.execute_reply.started":"2023-07-12T06:47:01.008625Z","shell.execute_reply":"2023-07-12T06:47:01.014289Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"tensor([[-3.9194e-01,  1.3294e+00, -1.5366e+00],\n        [ 2.8090e+00,  6.6517e-01, -1.2615e+00],\n        [ 1.1996e+00,  1.0735e+00,  1.1622e-01],\n        [ 3.6831e-01,  1.7502e-03,  7.1452e-01]])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(x[:,1]) #显示x的第二列","metadata":{"execution":{"iopub.status.busy":"2023-07-12T06:47:41.388317Z","iopub.execute_input":"2023-07-12T06:47:41.388642Z","iopub.status.idle":"2023-07-12T06:47:41.398049Z","shell.execute_reply.started":"2023-07-12T06:47:41.388619Z","shell.execute_reply":"2023-07-12T06:47:41.396445Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"tensor([ 0.3705, -0.1416,  1.0646, -0.4728])\n","output_type":"stream"}]},{"cell_type":"code","source":"y=x[0,:]\ny+=1\nprint(y)\nprint(x[0,:]) #原来的tensor也被更改了\n\n##索引出来的结果与原来的数据共享内存，修改一个时另外一个也会被修改","metadata":{"execution":{"iopub.status.busy":"2023-07-12T06:48:34.366792Z","iopub.execute_input":"2023-07-12T06:48:34.367194Z","iopub.status.idle":"2023-07-12T06:48:34.374685Z","shell.execute_reply.started":"2023-07-12T06:48:34.367166Z","shell.execute_reply":"2023-07-12T06:48:34.373730Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"tensor([-0.1805,  1.3705, -0.9573])\ntensor([-0.1805,  1.3705, -0.9573])\n","output_type":"stream"}]},{"cell_type":"code","source":"#改变tensor的大小或者形状时，使用torch.view\nx=torch.randn(4,4)\nprint(x)\ny=x.view(16) #将x变为16维的\nprint(y)\nz=x.view(-1,8) #-1指的是这一维的维数由其他维度决定\n#即z为8列的，行数则会相应的变味2\nprint(z)\nprint(x.size(),y.size(),z.size())","metadata":{"execution":{"iopub.status.busy":"2023-07-12T06:51:00.594627Z","iopub.execute_input":"2023-07-12T06:51:00.594986Z","iopub.status.idle":"2023-07-12T06:51:00.604727Z","shell.execute_reply.started":"2023-07-12T06:51:00.594958Z","shell.execute_reply":"2023-07-12T06:51:00.603118Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"tensor([[-5.7076e-01, -1.0511e+00,  1.6455e+00,  2.2046e-01],\n        [-1.6691e+00,  1.0400e+00,  6.6054e-01,  1.1049e+00],\n        [-6.9345e-01, -1.5643e-03, -3.5746e-01, -1.2440e+00],\n        [ 1.5544e-01,  4.6448e-01, -4.4565e-01,  9.2924e-01]])\ntensor([-5.7076e-01, -1.0511e+00,  1.6455e+00,  2.2046e-01, -1.6691e+00,\n         1.0400e+00,  6.6054e-01,  1.1049e+00, -6.9345e-01, -1.5643e-03,\n        -3.5746e-01, -1.2440e+00,  1.5544e-01,  4.6448e-01, -4.4565e-01,\n         9.2924e-01])\ntensor([[-5.7076e-01, -1.0511e+00,  1.6455e+00,  2.2046e-01, -1.6691e+00,\n          1.0400e+00,  6.6054e-01,  1.1049e+00],\n        [-6.9345e-01, -1.5643e-03, -3.5746e-01, -1.2440e+00,  1.5544e-01,\n          4.6448e-01, -4.4565e-01,  9.2924e-01]])\ntorch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"view()返回的新tensor与源tensor共享内存<br>\n即view仅仅是改变了对这个张量的观察角度，当改变其中一个时，另外一个也会改变","metadata":{}},{"cell_type":"code","source":"x+=1 #后面x与y同时加1\nprint(x)\nprint(y)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T06:52:42.095469Z","iopub.execute_input":"2023-07-12T06:52:42.095817Z","iopub.status.idle":"2023-07-12T06:52:42.104916Z","shell.execute_reply.started":"2023-07-12T06:52:42.095790Z","shell.execute_reply":"2023-07-12T06:52:42.103242Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"tensor([[1.4292, 0.9489, 3.6455, 2.2205],\n        [0.3309, 3.0400, 2.6605, 3.1049],\n        [1.3066, 1.9984, 1.6425, 0.7560],\n        [2.1554, 2.4645, 1.5543, 2.9292]])\ntensor([1.4292, 0.9489, 3.6455, 2.2205, 0.3309, 3.0400, 2.6605, 3.1049, 1.3066,\n        1.9984, 1.6425, 0.7560, 2.1554, 2.4645, 1.5543, 2.9292])\n","output_type":"stream"}]},{"cell_type":"code","source":"x=torch.randn(1)\nprint(x)\nprint(x.item())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 广播机制：当对两个形状不同的tensor按照元素进行计算时，可能会触发广播机制\n<br>先适当复制元素使得这两个tensor形状相同后再按照元素进行计算","metadata":{}},{"cell_type":"code","source":"x=torch.arange(1,3) #arange(s,e)为从[s,e)\nprint(x)\nx=torch.arange(1,3).view(1,2) #生成1行2列的tensor\nprint(x)\ny=torch.arange(1,4).view(3,1) #生成3行1列的tensor\nprint(y)\nprint(x+y)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T06:57:12.401494Z","iopub.execute_input":"2023-07-12T06:57:12.402033Z","iopub.status.idle":"2023-07-12T06:57:12.408853Z","shell.execute_reply.started":"2023-07-12T06:57:12.402003Z","shell.execute_reply":"2023-07-12T06:57:12.408168Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"tensor([1, 2])\ntensor([[1, 2]])\ntensor([[1],\n        [2],\n        [3]])\ntensor([[2, 3],\n        [3, 4],\n        [4, 5]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"x 和 y 分别是1行2列和3行1列的矩阵，如果要计算 x + y ，那么 x 中第一行的2个元素被广播 (复制)到了第二行和第三行，⽽ y 中第⼀列的3个元素被广播(复制)到了第二列。如此，就可以对2 个3行2列的矩阵按元素相加。","metadata":{}},{"cell_type":"markdown","source":"#### 自动求导","metadata":{}},{"cell_type":"markdown","source":"神经网络的核心为`autograd包`<br>\n该包为张量上的所有操作提供了自动求导机制<br>\n`torch.Tensor`为这个包的核心类：当设置`.requires_grad`为True时，它将会追踪对于该张量的所有操作；当完成计算后可以调用`.backward()`来自动计算所有的梯度；这个张量的所有梯度会自动累加到`.grad`属性","metadata":{}},{"cell_type":"code","source":"x=torch.randn(3,3,requires_grad=True)\nprint(x.grad_fn)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T07:02:58.259461Z","iopub.execute_input":"2023-07-12T07:02:58.259830Z","iopub.status.idle":"2023-07-12T07:02:58.265601Z","shell.execute_reply.started":"2023-07-12T07:02:58.259801Z","shell.execute_reply":"2023-07-12T07:02:58.264584Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"None\n","output_type":"stream"}]},{"cell_type":"markdown","source":"如果需要计算导数，可以在`Tensor`上调用`.backward()`。<br>\n如果 Tensor 是一个标量(即它包含一个元素的数据），则不需要为 backward() 指定任何参数，但是如果它有更多的元素，则需要指定一个`gradient参数`，该参数是形状匹配的张量。","metadata":{}},{"cell_type":"code","source":"#创建一个张量并设置requires_grad=True用来追踪其计算历史\nx=torch.ones(2,2,requires_grad=True)\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T07:04:34.806769Z","iopub.execute_input":"2023-07-12T07:04:34.807070Z","iopub.status.idle":"2023-07-12T07:04:34.814571Z","shell.execute_reply.started":"2023-07-12T07:04:34.807049Z","shell.execute_reply":"2023-07-12T07:04:34.812984Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"#对这个张量做一次运算\ny=x**2\nprint(y)\n#y是计算的结果，所以它有grad_fn属性\nprint(y.grad_fn)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T07:05:18.870742Z","iopub.execute_input":"2023-07-12T07:05:18.871123Z","iopub.status.idle":"2023-07-12T07:05:18.878424Z","shell.execute_reply.started":"2023-07-12T07:05:18.871093Z","shell.execute_reply":"2023-07-12T07:05:18.877396Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"tensor([[1., 1.],\n        [1., 1.]], grad_fn=<PowBackward0>)\n<PowBackward0 object at 0x7fe00e1848b0>\n","output_type":"stream"}]},{"cell_type":"code","source":"#对y进行更多的操作\nz=y*y*3\nout=z.mean()\nprint(z,out)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T07:05:42.907283Z","iopub.execute_input":"2023-07-12T07:05:42.907620Z","iopub.status.idle":"2023-07-12T07:05:42.918509Z","shell.execute_reply.started":"2023-07-12T07:05:42.907597Z","shell.execute_reply":"2023-07-12T07:05:42.917276Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"tensor([[3., 3.],\n        [3., 3.]], grad_fn=<MulBackward0>) tensor(3., grad_fn=<MeanBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"a=torch.randn(2,2) #缺失情况下默认requires_grad=False\na=((a*3)/(a-1))\nprint(a.requires_grad)\na.requires_grad_(True)\nprint(a.requires_grad)\nb=(a*a).sum()\nprint(b.grad_fn)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T07:07:28.938084Z","iopub.execute_input":"2023-07-12T07:07:28.938436Z","iopub.status.idle":"2023-07-12T07:07:28.948013Z","shell.execute_reply.started":"2023-07-12T07:07:28.938409Z","shell.execute_reply":"2023-07-12T07:07:28.947213Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"False\nTrue\n<SumBackward0 object at 0x7fe0313f7520>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 梯度\n现在开始进行反向传播\nout为一个标量，因此out.backward()与out.backward(torch.tensor(1.))等价","metadata":{}},{"cell_type":"code","source":"out.backward()","metadata":{"execution":{"iopub.status.busy":"2023-07-12T07:11:00.996738Z","iopub.execute_input":"2023-07-12T07:11:00.997176Z","iopub.status.idle":"2023-07-12T07:11:01.010598Z","shell.execute_reply.started":"2023-07-12T07:11:00.997124Z","shell.execute_reply":"2023-07-12T07:11:01.009083Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"print(x.grad) #导数d(out)/dx （雅可比矩阵）","metadata":{"execution":{"iopub.status.busy":"2023-07-12T07:11:09.376840Z","iopub.execute_input":"2023-07-12T07:11:09.377420Z","iopub.status.idle":"2023-07-12T07:11:09.385478Z","shell.execute_reply.started":"2023-07-12T07:11:09.377369Z","shell.execute_reply":"2023-07-12T07:11:09.384075Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"tensor([[3., 3.],\n        [3., 3.]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。","metadata":{}},{"cell_type":"markdown","source":"`torch.autograd`可以用于计算一些雅可比矩阵的乘积","metadata":{}},{"cell_type":"code","source":"#再次进行反向传播\nout2=x.sum()\nout2.backward()\nprint(x.grad)\n\nout3=x.sum()\nx.grad.data.zero_()\nout3.backward()\nprint(x.grad)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T07:13:08.546338Z","iopub.execute_input":"2023-07-12T07:13:08.546699Z","iopub.status.idle":"2023-07-12T07:13:08.555818Z","shell.execute_reply.started":"2023-07-12T07:13:08.546674Z","shell.execute_reply":"2023-07-12T07:13:08.554685Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"tensor([[4., 4.],\n        [4., 4.]])\ntensor([[1., 1.],\n        [1., 1.]])\n","output_type":"stream"}]},{"cell_type":"code","source":"x=torch.randn(3,requires_grad=True)\nprint(x)\n\ny=x*2\ni=0\nwhile y.data.norm()<1000:\n    y=y*2\n    i=i+1\nprint(y)\nprint(i)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T07:16:28.757521Z","iopub.execute_input":"2023-07-12T07:16:28.757863Z","iopub.status.idle":"2023-07-12T07:16:28.773384Z","shell.execute_reply.started":"2023-07-12T07:16:28.757838Z","shell.execute_reply":"2023-07-12T07:16:28.771570Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"tensor([ 0.6156, -2.5324,  2.1429], requires_grad=True)\ntensor([  315.2114, -1296.6089,  1097.1757], grad_fn=<MulBackward0>)\n8\n","output_type":"stream"}]},{"cell_type":"markdown","source":"torch.autograd 不能直接计算完整的雅可比矩阵，但是如果我们只想要雅可比向量积，只需将这个向量作为参数传给 backward：","metadata":{}},{"cell_type":"code","source":"v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\ny.backward(v)\n\nprint(x.grad)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T07:21:22.954771Z","iopub.execute_input":"2023-07-12T07:21:22.955517Z","iopub.status.idle":"2023-07-12T07:21:22.962461Z","shell.execute_reply.started":"2023-07-12T07:21:22.955486Z","shell.execute_reply":"2023-07-12T07:21:22.961620Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"也可以通过将代码块包装在 with torch.no_grad(): 中，来阻止 autograd 跟踪设置了.requires_grad=True的张量的历史记录。","metadata":{}},{"cell_type":"code","source":"print(x.requires_grad)\nprint((x ** 2).requires_grad)\n\nwith torch.no_grad():\n    print((x ** 2).requires_grad)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T07:22:55.463413Z","iopub.execute_input":"2023-07-12T07:22:55.464141Z","iopub.status.idle":"2023-07-12T07:22:55.469994Z","shell.execute_reply.started":"2023-07-12T07:22:55.464098Z","shell.execute_reply":"2023-07-12T07:22:55.468327Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"True\nTrue\nFalse\n","output_type":"stream"}]},{"cell_type":"markdown","source":"如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录(即不会影响反向传播)， 那么我们可以对 tensor.data 进行操作。","metadata":{}},{"cell_type":"code","source":"x = torch.ones(1,requires_grad=True)\n\nprint(x.data) # 还是一个tensor\nprint(x.data.requires_grad) # 但是已经是独立于计算图之外\n\ny = 2 * x\nx.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n\ny.backward()\nprint(x) # 更改data的值也会影响tensor的值 \nprint(x.grad)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T07:23:13.392472Z","iopub.execute_input":"2023-07-12T07:23:13.392783Z","iopub.status.idle":"2023-07-12T07:23:13.404894Z","shell.execute_reply.started":"2023-07-12T07:23:13.392761Z","shell.execute_reply":"2023-07-12T07:23:13.403701Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"tensor([1.])\nFalse\ntensor([100.], requires_grad=True)\ntensor([2.])\n","output_type":"stream"}]}]}